---
title: "How to run local models with llama.cpp in windows"
description: "How to run local models with llama.cpp in windows to be able to run actuall usable coding models locally with low vram"
pubDate: 2026-01-18
draft: false
---


Welcome to the blog. This space is f 

## Why Astro + MDX

- **Speed**: content-first and fast builds.
- **Control**: custom layouts without framework bloat.
- **Focus**: writing first, features second.

```ts
export function greet(name: string) {
  return `Hello, ${name}`;
}
```
